{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottom-up Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import PyPDF2\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_text_from_pdf(countries_legend, country, pg_from, pg_to, dimension, save_txt = False):\n",
    "    pg_from = pg_from - 1\n",
    "    pg_to = pg_to - 1\n",
    "    doc_name = countries_legend.loc[countries_legend['country'] == country, 'doc_name'].iloc[0]\n",
    "    pdf_file = open(\"Energy National Plans/\" + doc_name + \".pdf\", \"rb\")\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    all_text = ''\n",
    "    for i in range(pg_to - pg_from):\n",
    "        doc = pdf_reader.pages[pg_from + i]\n",
    "        all_text = all_text + doc.extract_text()\n",
    "\n",
    "    if save_txt:\n",
    "                txt = open('output/' + dimension + '_txt/' + country + \".txt\", 'w', encoding=\"utf-8\")\n",
    "                txt.write(all_text)\n",
    "                txt.close()\n",
    "    return all_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Function extract_text_from_pdf\n",
    "    Returns full text from any pdf by providing pdf_path\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = pdf_reader.pages\n",
    "\n",
    "        for page_num in range(len(num_pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "def read_from_pdf(files, countries):\n",
    "    \"\"\"\n",
    "    Function read_from_pdf\n",
    "    Returns a list containing all NECP texts\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for i in tqdm.tqdm(range(len(files)), desc='Reading pdf file'):\n",
    "        t = extract_text_from_pdf('Energy National Plans/' + files[i])\n",
    "        texts.append(t)\n",
    "\n",
    "        txt = open('output/Full_txt/' + countries[i] + \".txt\", 'w', encoding=\"utf-8\")\n",
    "        txt.write(t)\n",
    "        txt.close()\n",
    "    \n",
    "    return texts\n",
    "\n",
    "def read_by_section(dimension, countries_legend, countries):\n",
    "    \"\"\"\n",
    "    Function read_by_section\n",
    "    Returns text from a desired section of NECP. \n",
    "    countries_legend.xlsx must be filled out before execution, providing which pages should the algorithm read\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for i in range(len(countries)):\n",
    "        pg_from = int(countries_legend.loc[countries_legend['country'] == countries[i], 'page_from'].iloc[0])\n",
    "        pg_to   = int(countries_legend.loc[countries_legend['country'] == countries[i], 'page_to'].iloc[0]) + 1  \n",
    "        text = get_text_from_pdf(countries_legend, countries[i], pg_from, pg_to, dimension, save_txt = False)\n",
    "        texts.append(text) \n",
    "\n",
    "    return texts\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\" \n",
    "    Function clean_text\n",
    "    Returns a sentence without extra-spaces, punctuation signs, etc\n",
    "    \"\"\"\n",
    "    # Normalize tabs and remove newlines\n",
    "    text = str(text).replace('\\t', ' ').replace('\\n', '')\n",
    "    # Remove all characters except A-Z and a dot.\n",
    "    text = re.sub(\"[^a-zA-Z\\.]\", \" \", text)\n",
    "    text = text.replace(\".\", \"\")\n",
    "    # Normalize spaces to 1\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    # Strip trailing and leading spaces\n",
    "    text = text.strip()\n",
    "    # Normalize all charachters to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Filter out words with less than 2 characters\n",
    "    words = text.split()\n",
    "    text = [word for word in words if len(word) >= 2]\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def norm_text(text):\n",
    "    \"\"\"\n",
    "    Function norm_text\n",
    "    Returns words after applying lemmatization\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    corpus_norm = []\n",
    "    for i in range(len(text)):\n",
    "        words = word_tokenize(text[i])\n",
    "        clean_sent = []\n",
    "        for j in range(len(words)):\n",
    "            clean_sent.append(lemmatizer.lemmatize(words[j]))\n",
    "        corpus_norm.append(' '.join(clean_sent))\n",
    "    return corpus_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Read text and create clean corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1- Select which method you want to use to extract the information from NECP. Options:\n",
    " - 1: Read a single dimensions from all plans (the name must be indicated in countries_legend tabs)\n",
    " - 2: Read all sections of NECPs and store them in folder **output/Full_txt/**\n",
    " - 3: If option 2 has been executed previously, just read output stored in folder\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['Austria','Belgium','Bulgary','Croatia','Cyprus','Czechia','Denmark','Estonia','Finland','France','Germany','Greece','Hungary','Ireland',\n",
    "'Italy','Latvia','Lithuania','Luxemburg','Malta','Netherlands','Poland','Portugal','Romania','Slovakia','Slovenia','Spain','Sweden']\n",
    "\n",
    "files = [file for file in os.listdir('Energy National Plans/') if file.lower().endswith('.pdf')]\n",
    "\n",
    "# 1- Read by Dimension\n",
    "dimension = 'Energy_security_1'\n",
    "countries_legend = pd.read_excel(\"countries_legend.xlsx\", sheet_name = dimension)\n",
    "texts = read_by_section(dimension, countries_legend, countries)\n",
    "\n",
    "# 2- Read from pdf\n",
    "# texts = read_from_pdf(files, countries)\n",
    "\n",
    "# 3- Read from txt\n",
    "# texts = []\n",
    "# files = os.listdir('output/Full_txt/')\n",
    "# for file in files:\n",
    "#     with open('output/Full_txt/' + file, 'r', encoding=\"utf-8\") as f:\n",
    "#         texts.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get stop-words in the english language. Next tokenize each text by sentence, while cleaning, normalizing, and tokenizing by word. The output must be a list containing each sentence tokenized by words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "data = []\n",
    "for text in texts:\n",
    "    sents = sent_tokenize(text)\n",
    "    sents = [clean_text(sen) for sen in sents]\n",
    "    sents = [sent for sent in sents if len(word_tokenize(sent)) > 10]\n",
    "    sents = norm_text(sents)\n",
    "    \n",
    "    new_sents = []\n",
    "    for sent in sents:\n",
    "        new_sents_1 = []\n",
    "        for word in word_tokenize(sent):\n",
    "            new_sents_1.append(word)\n",
    "        data.append(new_sents_1)\n",
    "\n",
    "all_corpus = []\n",
    "for i in range(len(texts)):\n",
    "    corpus = sent_tokenize(texts[i])\n",
    "    corpus_clean = [clean_text(sen) for sen in corpus]\n",
    "    corpus_clean = [sentence for sentence in corpus_clean if len(word_tokenize(sentence)) > 10]\n",
    "    corpus_norm = norm_text(corpus_clean)\n",
    "    \n",
    "    all_corpus.append(corpus_norm)\n",
    "\n",
    "input_data = []\n",
    "for i in range(len(all_corpus)):\n",
    "    for j in range(len(all_corpus[i])):\n",
    "        words = word_tokenize(all_corpus[i][j])\n",
    "        input_data.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "texts = input_data\n",
    "bigram = gensim.models.Phrases(texts, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[texts], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Implement wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Select number of country\n",
    "n_country = 25 \n",
    "foo = [' '.join(process_words(all_corpus[n_country])[i]) for i in tqdm.tqdm(range(len(all_corpus[n_country])))]\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(foo))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Compute LDA model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ### 3.1- Find optimal number of topics between 2 and 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "for num_topics in range(2, 6, 1):\n",
    "    model = gensim.models.ldamodel.LdaModel(corpus = corpus,\n",
    "                                           id2word = id2word,\n",
    "                                           num_topics = num_topics, \n",
    "                                           random_state = 100,\n",
    "                                           update_every = 1,\n",
    "                                           chunksize = 10,\n",
    "                                           passes = 10,\n",
    "                                           alpha = 'symmetric',\n",
    "                                           iterations = 100,\n",
    "                                           per_word_topics = True)\n",
    "    model_list.append(model)\n",
    "    coherencemodel = CoherenceModel(model = model, texts = data_ready, dictionary = id2word, coherence = 'c_v')\n",
    "    coh = coherencemodel.get_coherence()\n",
    "    coherence_values.append(coh)\n",
    "    print(coh)\n",
    "    \n",
    "x = range(2, 6, 1)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ### 3.2- Select number of topics and execute LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SELECT N_TOPICS #####\n",
    "N_topics = 3\n",
    "##### SELECT N_TOPICS #####\n",
    "\n",
    "# Build LDA model\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus,\n",
    "                                           id2word = id2word,\n",
    "                                           num_topics = N_topics, \n",
    "                                           random_state = 100,\n",
    "                                           update_every = 1,\n",
    "                                           chunksize = 10,\n",
    "                                           passes = 10,\n",
    "                                           alpha = 'symmetric',\n",
    "                                           iterations = 100,\n",
    "                                           per_word_topics = True)\n",
    "\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Tópico {idx}:\", topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_df = pd.DataFrame()\n",
    "for i, row_list in enumerate(lda_model[corpus]):\n",
    "    row = row_list[0] if lda_model.per_word_topics else row_list \n",
    "    row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "    for j, (topic_num, prop_topic) in enumerate(row):\n",
    "        if j == 0:\n",
    "            wp = lda_model.show_topic(topic_num)\n",
    "            topic_keywords = \", \".join([word for word, prop in wp])\n",
    "            sent_topics_df = pd.concat([sent_topics_df, pd.DataFrame([pd.Series([int(topic_num), round(prop_topic,4), topic_keywords])])], ignore_index=True)\n",
    "        else:\n",
    "            break\n",
    "sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "contents = pd.Series(texts)\n",
    "sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "\n",
    "df_topic_sents_keywords = sent_topics_df\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "df_dominant_topic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ### 3.3- Implement topic distributiona analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = []\n",
    "for i in range(len(countries)):\n",
    "    for j in range(len(all_corpus[i])):\n",
    "        aux.append(countries[i])\n",
    "df_dominant_topic['aux'] = aux\n",
    "distributions = pd.DataFrame()\n",
    "\n",
    "topics = range(0, N_topics, 1)\n",
    "for topic in topics:\n",
    "    nss = []\n",
    "    for country in countries:\n",
    "        df_aux = df_dominant_topic[(df_dominant_topic['aux'] == country) & (df_dominant_topic['Dominant_Topic'] == topic)]\n",
    "        len1 = len(df_dominant_topic[df_dominant_topic['aux'] == country])\n",
    "        len2 = len(df_dominant_topic[(df_dominant_topic['aux'] == country) & (df_dominant_topic['Dominant_Topic'] == topic)])\n",
    "        nss.append(len2/len1)\n",
    "    distributions[str(topic)] = nss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if N_topics == 4:\n",
    "    discursos = ['first']*27+['second']*27+['third']*27+['fourth']*27\n",
    "    scores = distributions['0'].to_list() + distributions['1'].to_list() + distributions['2'].to_list() + distributions['3'].to_list()\n",
    "elif N_topics == 3:\n",
    "    discursos = ['first']*27+['second']*27+['third']*27\n",
    "    scores = distributions['0'].to_list() + distributions['1'].to_list() + distributions['2'].to_list()\n",
    "elif N_topics == 2:\n",
    "    discursos = ['first']*27+['second']*27\n",
    "    scores = distributions['0'].to_list() + distributions['1'].to_list()\n",
    "\n",
    "df = pd.DataFrame({'Discourses': discursos, 'scores': scores})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "palette = sns.color_palette(\"Set2\")\n",
    "sns.violinplot(x=\"Discourses\", y=\"scores\", data=df, ax=ax, linewidth=.8, inner='point', palette='muted', )\n",
    "sns.boxplot(x=\"Discourses\", y=\"scores\", data=df, ax=ax, width=0.3, linewidth=.8, palette='bright')\n",
    "\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.set_title(\"Topic probabilities among EU member states\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a partir d'aqui hi ha proves/funcionalitats que es poden crear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(2)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_ready for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    # ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.gridspec as grid_spec\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# import pyarrow.parquet as pq\n",
    "# import pyarrow as pa\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "#---NLP packages--------------------\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "#----process string-------\n",
    "import string\n",
    "import re\n",
    "\n",
    "#---network visualization-----------\n",
    "import re\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # alternatiu\n",
    "# input_data = []\n",
    "# for j in range(len(all_corpus[25])):\n",
    "#     words = word_tokenize(all_corpus[25][j])\n",
    "#     input_data.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in range(len(input_data)):\n",
    "    data_1 = []\n",
    "    for j in range(len(input_data[i])):\n",
    "        if input_data[i][j] not in stop_words:\n",
    "            data_1.append(input_data[i][j])\n",
    "    data.append(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = []\n",
    "for words in data:\n",
    "    words_ = list(set(words))\n",
    "    for i in range(len(words_)-1):\n",
    "        for j in range(i+1, len(words_)):\n",
    "            word_i = words_[i]\n",
    "            word_j = words_[j]\n",
    "            if word_i < word_j:\n",
    "                word_pairs.append([word_i, word_j])\n",
    "            else:\n",
    "                word_pairs.append([word_i, word_j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_pairs = pd.DataFrame(data = word_pairs, columns=['word1', 'word2'])\n",
    "word_pairs_count = df_word_pairs.groupby(['word1', 'word2']).size()\n",
    "word_pairs_count = word_pairs_count.sort_values().tail(300).reset_index()\n",
    "word_pairs_count.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of lists containing bigrams \n",
    "terms_bigram = [list(bigrams(words)) for words in data]\n",
    "\n",
    "# View bigrams for the first assay\n",
    "print('View bigrams for the first assay')\n",
    "print(terms_bigram[0][:5])\n",
    "\n",
    "# Flatten list of bigrams in clean text\n",
    "bigram_list = list(itertools.chain(*terms_bigram))\n",
    "\n",
    "# Create counter of words in clean bigrams\n",
    "bigram_counts = collections.Counter(bigram_list)\n",
    "\n",
    "bigram_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of lists containing bigrams \n",
    "terms_3gram = [list(ngrams(words, 3)) for words in data]\n",
    "\n",
    "# View bigrams for the first assay\n",
    "print('View N-grams (N=3) for the first assay')\n",
    "print(terms_3gram[0][:5])\n",
    "\n",
    "\n",
    "gram3_list = list(itertools.chain(*terms_3gram))\n",
    "\n",
    "# Create counter of words in clean bigrams\n",
    "gram3_counts = collections.Counter(gram3_list)\n",
    "\n",
    "gram3_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network plot \n",
    "G = nx.Graph()\n",
    "\n",
    "# Create connections between nodes\n",
    "for v in bigram_counts.most_common(30):\n",
    "    G.add_edge(v[0][0], v[0][1], weight=(v[1] * 10))\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "\n",
    "pos = nx.spring_layout(G, k=8)\n",
    "\n",
    "d = dict(nx.degree(G))\n",
    "edges = G.edges()\n",
    "weights = [G[u][v]['weight']/1000 for u,v in edges]\n",
    "# Plot networks\n",
    "nx.draw_networkx(G, pos,\n",
    "                 font_size=16,\n",
    "                 width=weights,\n",
    "                 node_size = [v * 200 for v in d.values()], \n",
    "                 edge_color='grey',\n",
    "                 #node_color='tomato',\n",
    "                 with_labels = True,\n",
    "                 ax=ax)\n",
    "\n",
    "ax.set_title('Bigram Network', \n",
    "             fontdict={'fontsize': 26,\n",
    "            'fontweight': 'bold',\n",
    "            'color': 'salmon', \n",
    "            'verticalalignment': 'baseline',\n",
    "            'horizontalalignment': 'center'}, \n",
    "             loc='center')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network plot \n",
    "G = nx.Graph()\n",
    "\n",
    "\n",
    "for _, row in word_pairs_count.iterrows():\n",
    "    G.add_edge(row['word1'], row['word2'], weight=row[0])\n",
    "    \n",
    "pos_kkl = nx.kamada_kawai_layout(G)\n",
    "f, ax = plt.subplots(figsize=(16, 16))\n",
    "\n",
    "\n",
    "d = dict(nx.degree(G))\n",
    "edges = G.edges()\n",
    "weights = [G[u][v]['weight']/1000 for u,v in edges]\n",
    "\n",
    "nx.draw(G, pos_kkl, \n",
    "        with_labels=True, \n",
    "        node_size=[v * 100 for v in d.values()],\n",
    "        nodelist=d.keys(),  \n",
    "        width=weights, \n",
    "        edge_color='grey', #node_color=list(df_skills_stats['core_number']), cmap=\"coolwarm_r\", \n",
    "        alpha=0.9,\n",
    "       )\n",
    "#node_labels = nx.draw_networkx_labels(G, pos_kkl, labels, font_size=10)\n",
    "# Set title\n",
    "ax.set_title('Word Co-occurrence Network', \n",
    "             fontdict={'fontsize': 26,\n",
    "            'fontweight': 'bold',\n",
    "            'color': 'salmon', \n",
    "            'verticalalignment': 'baseline',\n",
    "            'horizontalalignment': 'center'}, \n",
    "             loc='center')\n",
    "# Set edge color\n",
    "plt.gca().collections[0].set_edgecolor(\"#000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dominant_topic.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 0\n",
    "primer = []\n",
    "segon  = []\n",
    "tercer = []\n",
    "# quart  = []\n",
    "\n",
    "for i in range(len(countries)): #len(countries)\n",
    "    # print(i)\n",
    "    n2 = len(all_corpus[i]) + n1\n",
    "    df = df_dominant_topic.iloc[n1:n2]\n",
    "    df1 = df['Dominant_Topic'].value_counts()\n",
    "    # print(df1)\n",
    "    primer.append(df1[0]) if 0 in df['Dominant_Topic'].value_counts() else primer.append(0)\n",
    "    segon.append(df1[1])  if 1 in df['Dominant_Topic'].value_counts() else segon.append(0)\n",
    "    tercer.append(df1[2]) if 2 in df['Dominant_Topic'].value_counts() else tercer.append(0)\n",
    "    # quart.append(df1[3])  if 3 in df['Dominant_Topic'].value_counts() else quart.append(0)\n",
    "    n1 += len(all_corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe_resum = pd.DataFrame({'country': countries, 'Discurs1': primer, 'Discurs2': segon, 'Discurs3': tercer, 'Discurs4': quart})\n",
    "dataframe_resum = pd.DataFrame({'country': countries, 'Discurs1': primer, 'Discurs2': segon, 'Discurs3': tercer})\n",
    "\n",
    "dataframe_resum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "\n",
    "for i in tqdm.tqdm(range(10000)):\n",
    "    keys.append(list(bigram_counts.items())[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "b = []\n",
    "\n",
    "for key in keys:\n",
    "    a.append(key[0])\n",
    "    b.append(key[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'bigram': a, 'num': b})\n",
    "df = df.sort_values(by='num', ascending=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    if 'europe' in df['bigram'].iloc[i]:\n",
    "        print(df['bigram'].iloc[i], df['num'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dict(bigram_counts)\n",
    "\n",
    "keys = list(a.keys())\n",
    "values = list(a.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    if 'gas' in keys[i]:\n",
    "        print(keys[i])\n",
    "        print(values[i])\n",
    "    # if 'gas' in str(keys[i]):\n",
    "    #     print(keys[i])\n",
    "    #     print(values[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dict(gram3_counts)\n",
    "\n",
    "keys = list(b.keys())\n",
    "values = list(b.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    # if 'gas' in keys[i]:\n",
    "    #     print(keys[i])\n",
    "    #     print(values[i])\n",
    "    if 'gas' in str(keys[i]):\n",
    "        print(keys[i])\n",
    "        print(values[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = []\n",
    "for bigram in list(a):\n",
    "    if 'gas' in str(bigram):\n",
    "        s.append(bigram)\n",
    "\n",
    "bigram_list = list(itertools.chain(*s))\n",
    "\n",
    "# Create counter of words in clean bigrams\n",
    "bigram_counts = collections.Counter(bigram_list)\n",
    "\n",
    "bigram_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
